\documentclass[12pt, a4paper]{elsarticle}

% ------------ packages -------------

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{todonotes}
\usepackage{etoolbox}
\usepackage{url}
%\usepackage{tikz}
%\usetikzlibrary{shapes.misc,fit}

\usepackage[bookmarks]{hyperref}

% ------------ custom defs -------------

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bs#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\Rsys}{R_\text{sys}}
\newcommand{\lRsys}{\ul{R}_\text{sys}}
\newcommand{\uRsys}{\ol{R}_\text{sys}}

\newcommand{\Fsys}{F_\text{sys}}
\newcommand{\lFsys}{\ul{F}_\text{sys}}
\newcommand{\uFsys}{\ol{F}_\text{sys}}

\def\Tsys{T_\text{sys}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}

\newcommand{\indic}{\mathbb{I}}

\newcommand{\ber}{\operatorname{Bernoulli}} 
\newcommand{\bin}{\operatorname{Binomial}}
\newcommand{\be}{\operatorname{Beta}} 
\newcommand{\bebin}{\operatorname{Beta-Binomial}} 

\def\tmax{t_\text{max}}
\def\tnow{t_\text{now}}
\def\tpnow{t^+_\text{now}}

\newcommand{\ptk}{p^k_t}

\input{nydefs.tex}

%\newcommand{\comments}[1]{{\small\color{gray} #1}}
\newtoggle{td}
\newcommand{\td}[1]{%
  \iftoggle{td}{%
    \todo[inline]{#1}%
  }{}%
}

% ------------ options -------------

\allowdisplaybreaks

\toggletrue{td} % show todo's
%\togglefalse{td} % hide todo's

%\biboptions{longnamesfirst,angle,semicolon}


\journal{IJAR}

\begin{document}

% ------------ frontmatter -------------

\begin{frontmatter}
\title{Bayesian Nonparametric System Reliability\\ using Sets of Priors}

\author[ein]{Gero Walter\fnref{fn1}}
\ead{g.m.walter@tue.nl}
\author[oxf]{Louis J.M. Aslett}
\ead{louis.aslett@stats.ox.ac.uk}
\author[dur]{Frank P.A. Coolen}
\ead{frank.coolen@durham.ac.uk}

\address[ein]{School of Industrial Engineering, Eindhoven University of Technology, Eindhoven, NL}
\address[oxf]{Department of Statistics, University of Oxford, Oxford, UK}
\address[dur]{Department of Mathematical Sciences, Durham University, Durham, UK}

\fntext[fn1]{Gero Walter was supported by the Dinalog project
``Coordinated Advanced Maintenance and Logistics Planning for the Process Industries'' (CAMPI).}

\begin{abstract}
Imprecise Bayesian nonparametric approach to system reliability with multiple types of components,
sets of priors through sets of canonical parameters,
leading to sets of system reliability functions,
reflection of prior-data conflict
\end{abstract}

\begin{keyword}
System reliability \sep
Survival signature \sep
Imprecise probability \sep
Bayesian Nonparametrics \sep
Prior-data conflict
\end{keyword}
\end{frontmatter}


% ------------ manuscript -------------

\section{Introduction}

(some sentences may be useful for the abstract instead)

System with components of $k=1,\ldots,K$ different types.
There are $m_k$ components of type $k$ in the system.
Components of the same type are i.i.d.\ and independent of components of other types.
Arbitrary system layout, i.e.\ any series / parallel combination, $k$ out of $n$ etc.

The survival signature allows for straightforward computation of system reliability
for systems with complex layouts and multiple component types.
It separates the (time-invariant) system structure from the time-dependent failure probabilities of components,
by partitioning the event $\Tsys > t$ according to the number of components of each type functioning at time $t$.

Based on expert assumptions for component failure distributions and component test data,
we make predictive inference for a system with components that are exchangeable
with the test components by calculating the system reliability function $\Rsys(t) = P(\Tsys > t)$.
Considering a discrete grid of time points $\{t_1, \ldots, \tmax\}$,
naturally a Bayesian model for the functioning probability at each time point arises (see Section~\ref{sec:nonparamapproach}).
To account for uncertainty in the choice of prior
and due to the inherent issue with prior-data conflict (see below / Section~\ref{sec:reparam}),
we propose an imprecise probability approach
based on sets of prior distributions defined through sets of canonical parameters.
This approach also allows to adequately model weak or partial prior information.
In particular, prior near ignorance can be modelled much more naturally than through usual so-called noninformative priors. 

%Our imprecise probability approach leads to sets of system reliability functions,
%where the magnitude of the set, i.e., the width of the posterior predictive functioning probability interval
%reflects the precision of the reliability estimate:
%a short range will indicate that we can quantify the reliability of the system quite precisely,
%while a large range will indicate that our (probabilistic) knowledge is rather shaky --
%remember that the reliability function is in essence a collection of probability statements.
Our imprecise probability approach leads, for each $t$ in some grid of time points ${\cal T}$,
to a posterior predictive probability interval for the event that the system will function at time $t$.
The width of these intervals reflects the precision of the probability statement:
a short range will indicate that we can quantify the system functioning probability quite precisely,
while a large range will indicate that our (probabilistic) knowledge is indeterminate.

A central advantage of our imprecise probability model is
that it leads to more cautious probability statements
in case of \emph{prior-data conflict} \cite[see, e.g.,][]{2006:evans}:
From the viewpoint of the expert, the observed failure times seem very surprising,
i.e., information from data is in conflict with prior assumptions.
This is most relevant when there is not enough data to overrule the prior;
as it is then unclear whether to put more trust to prior assumptions or to the observations,
posterior inferences should clearly reflect this state of uncertainty.

As we will show in Section~\ref{sec:reparam}, when taking the standard choice of a conjugate Beta prior,
prior-data conflict is ignored, as the spread of the posterior distribution does not increase in case of such a conflict,
ultimately conveying a false sense of certainty
by communicating that we can quantify the reliability of the system quite precisely when in fact we cannot do so.
In contrast, our method will indicate prior-data conflict by wider bounds for $\Rsys(t)$.
This behaviour is obtained by a specific choice for the set of priors (see Section~\ref{sec:setsofbetapriors})
which leads to larger sets of posterior distributions when prior knowledge and data are in conflict.

Prior-data conflict has recently also been considered by \citet{2015:bickel},
who, starting from a large class of reasonable models,
identifies a subset of adequate models,
selected according to a model adequacy measure that allows to reject a prior if it leads to prior-data conflict.



Our method produces a set of discrete system reliability functions,
or, for each time point $t$, an interval for the system survival probability,
that appropriately reflects uncertainty due to vague prior information, the amount of test data, and prior-data conflict.

***main contribution of the paper:
set of priors thing,
guidelines on how to choose the parameters;
this is easier in terms of $(\nz, \yz)$ than in terms of $(\alpha, \beta)$***

This paper implements the nonparametric approach described in Section~4 of \citet{2015:bayessurvsign}
and extends it to sets of priors.

The paper is organised as follows.
In Section~\ref{sec:survsign}, we describe ***


\section{Survival Signature}
\label{sec:survsign}

In the mathematical theory of reliability, the main focus is on the functioning of a system given the functioning, or not, 
of its components and the structure of the system. The mathematical concept which is central to this theory is the 
\emph{structure function} \citep{BP75}. For a system with $m$ components, let state vector 
$\underline{x} = (x_1,x_2,\ldots,x_m) \in \{0,1\}^m$, with $x_i=1$ if the $i$th component functions 
and $x_i=0$ if not. The labelling of the components is arbitrary but must be fixed to define $\underline{x}$. 
The structure function $\phi : \{0,1\}^m \rightarrow \{0,1\}$, defined for all possible $\underline{x}$, takes 
the value 1 if the system functions and 0 if the system does not function for state vector $\underline{x}$. 
Most practical systems are coherent, which means that $\phi(\underline{x})$ 
is non-decreasing in any of the components of $\underline{x}$, so system functioning cannot be improved by worse performance 
of one or more of its components. The assumption of coherent systems is also convenient from the perspective of uncertainty
quantification for system reliability. It is further logical to assume that $\phi(\underline{0})=0$ and $\phi(\underline{1})=1$, 
so the system fails if all its components fail and it functions if all its components function. 

For larger systems, working with the full structure function may be complicated, and one may particularly
only need a summary of the structure function in case the system has exchangeable components of one or more
types. We use the term `exchangeable components' to indicate that the failure times of the components in the system
are exchangeable \citep{DF74}. \citet{2012:survsign} introduced such a summary,
called the \emph{survival signature}, 
to facilitate reliability analyses for systems with multiple types of components. In case of just a single type of components, 
the survival signature is closely related to the system signature \citep{Sa07}, which is well-established and the topic of many
research papers during the last decade. However, generalization of the signature to systems with
multiple types of components is extremely complicated (as it involves ordering order statistics of different
distributions), so much so that it cannot be applied to most practical systems. In addition to the 
possible use for such systems, where the benefit only occurs if there are multiple components of the 
same types, the survival signature is arguably also easier to interpret than the signature. 

Consider a system with $K\ge 1$ types of components, with $m_k$ components of type $k \in \{1,\ldots,K\}$ and 
$\sum_{k=1}^K m_k = m$. Assume that the random failure times of components of the same type are exchangeable \citep{DF74}.
Due to the arbitrary ordering of the components in the state vector, components of the same type can be grouped together, 
leading to a state vector that can be written as 
$\underline{x} = (\underline{x}^1,\underline{x}^2,\ldots,\underline{x}^K)$, with 
$\underline{x}^k = (x^k_1,x^k_2,\ldots,x^k_{m_k})$ the sub-vector representing the states of the components of type $k$. 

The \emph{survival signature} for such a system, denoted by $\Phi(l_1,\ldots,l_K)$, with $l_k=0,1,\ldots,m_k$ 
for $k=1,\ldots,K$, is defined as the probability for the event that the system functions given that \emph{precisely} $l_k$ of its 
$m_k$ components of type $k$ function, for each $k\in \{1,\ldots,K\}$ \citep{2012:survsign}.
Essentially, this creates a $K$-dimensional partition for the event $\Tsys > t$, such that $\Rsys(t) = P(\Tsys > t)$
can be calculated using the law of total probability:
\begin{align}
\label{eq:rsyswithsurvsign}
P(\Tsys > t)
 &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} P(\Tsys > t \mid C^1_t = l_1,\ldots, C^K_t = l_K) \nonumber\\
 &  \hspace*{24ex}                        \times P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \nonumber\\
 &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K)
                                                 P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \,,
%                                                 \prod_{k=1}^K P(C^k_t = l_k) \,.
\end{align}
where $C^k_t \in \{0, 1, \ldots, m_k\}$ denotes
the random number of components of type $k$ functioning at time $t$. 

For calculating the survival signature based on the structure function, observe that
there are $\binom{m_k}{l_k}$ state vectors $\underline{x}^k$ with $\sum_{i=1}^{m_k} x^k_i = l_k$. Let $S^k_{l_k}$ 
denote the set of these state vectors for components of type $k$ and let $S_{l_1,\ldots,l_K}$ denote the set of 
all state vectors for the whole system for which $\sum_{i=1}^{m_k} x^k_i = l_k$, $k=1,\ldots,K$. Due to the 
exchangeability assumption for the failure times of the $m_k$ components of type $k$, all the state vectors 
$\underline{x}^k \in S^k_{l_k}$ are equally likely to occur, hence \citep{2012:survsign}
\begin{align}
\label{eq:surv-sig}
\Phi(l_1,\ldots,l_K)
 &= \left[ \prod_{k=1}^K \binom{m_k}{l_k}^{-1} \right] \times \sum_{\underline{x} \in S_{l_1,\ldots,l_K}} \phi(\underline{x})\,.
\end{align}
%
%Let $C^k_t \in \{0,1,\ldots,m_k\}$ denote the number of components of type $k$ in the system that function at time $t>0$.
%Then, for system failure time $\Tsys$,
%\begin{align*}
%P(\Tsys > t) &= \Rsys(t) = \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K}  \Phi(l_1,\ldots,l_K) P\Big(\bigcap_{k=1}^K \{C^k_t = l_k\}\Big)\,.
%\end{align*}
It should be emphasized that when using the survival signature,
there are no restrictions on dependence of the failure times of components of different types,
as the probability $P(\bigcap_{k=1}^K \{C^k_t = l_k\})$ can take any form of dependence into account,
for example one can include common-cause failures quite straightforwardly into this approach \cite{CCM15}. %\citep[see][]{CCM15}.
However, there is a substantial simplification
if one can assume that the failure times of components of different types are independent,
and even more so if one can assume that the failure times of components of type $k$ 
are conditionally independent and identically distributed with CDF $F_k(t)$.
With these assumptions, we get
\begin{align*}
\Rsys(t) &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \left[ \Phi(l_1,\ldots,l_K)
            \prod_{k=1}^K \left( \binom{m_k}{l_k} [F_k(t)]^{m_k-l_k} [1-F_k(t)]^{l_k} \right) \right]\,.
\end{align*}
We will employ both assumptions in this paper,
leading to $C^k_t$ having a Beta-Binomial distribution,
giving us a closed form expression for $P(C^k_t = l_k)$ for all $t$, $k$, and $l_k$.

The main advantage of the survival signature, in line with this property of the signature for systems with a single type of 
components \citep{Sa07}, is that the information about the system structure is fully 
separated from the information about functioning of the components, which simplifies related statistical inference as well as
considerations of optimal system design. In particular for study of system reliability over time, with the structure of the system, 
and hence the survival signature, not changing, this separation also enables relatively straightforward statistical inferences. 

There are several relatively straightforward generalizations of the use of the survival signature.
The probabilities for the numbers of functioning components can be generalized to lower and upper probabilities,
as e.g.\ done by \citet{CCMA14} within the nonparametric predictive inference (NPI) framework of statistics \citep{Co11},
where lower and upper probabilities for the events $C_k = l_k$
are inferred from test data on components of the same types as those in the system.
This is an approach that is also followed in the current paper, but with the use of generalized Bayesian inference instead of NPI.
Like \citet{CCMA14}, we will utilize the monotonicity of the survival signature for coherent systems
to simplify computations.


\section{Nonparametric Bayesian Approach for Component Reliability}
\label{sec:nonparamapproach}

Let us denote the random failure time of component number $i$ of type $k$ by $T^k_i$, $i = 1, \ldots, m_k$.
The failure time distribution can be written in terms of the cdf $F^k(t) = P(T^k_i \le t)$,
or in terms of the reliability function $R^k(t) = P(T^k_i > t) = 1 - F^k(t)$,
also known as the survival function.
For a nonparametric description of $R^k(t)$,
we consider a set of time points $t$, $t \in {\cal T} = \{t_1, \ldots, \tmax\}$.

At each time point $t$, the operational state of a single component of type $k$
is Bernoulli distributed (functioning: 1, failed: 0) with parameter $\ptk$, so that
\begin{align*}
P\big(\indic(T^k_i > t) = 1\big) &= \ptk\,, \\
P\big(\indic(T^k_i > t) = 0\big) &= 1 - \ptk\,,
\end{align*}
That is, $\indic(T^k_i > t) \sim \ber(\ptk)$, $i = 1, \ldots, m_k$, $t \in {\cal T}$.

The set of probabilities $\{ \ptk, t \in {\cal T}\}$
defines a discrete failure time distribution for components of type $k$ through
\begin{align*}
R^k(t_j) &= P(T^k > t_j) = p^k_{t_j},\ t_j = t_1, \ldots, \tmax\,.
\end{align*}
We can also express this failure time distribution through the probability mass function (pmf) and discrete hazard function,
\begin{align*}
f^k(t_j) &= P\big(T^k \in (t_j,t_{j+1}]\big) = p^k_{t_j} - p^k_{t_{j+1}}\,,\\ 
h^k(t_j) &= P\big(T^k \in (t_j,t_{j+1}]\mid T^k > t_j\big) = \frac{f^k(t_j)}{R^k(t_j)}\,. % or R^k(t_{j-1}) ???
\end{align*}
The time grid $\cal T$ can be chosen to be appropriately dense for the application at hand,
with the natural extension between grid points by taking $R^k(\cdot)$ to be the right continuous step function induced by the grid values,
$R^k(t) = p^k_{t_j}, t \in [t_j, t_{j+1})$,
or by taking $p^k_{t_j}$ and $p^k_{t_{j+1}}$ as upper and lower bounds for $R^k(t)$, $t \in [t_j, t_{j+1})$.

\td{***illustrate discrete distribution with a graph of R, f, h?***}

The independence assumption for components of the same type immediately implies that 
the number of functioning components of type $k$ in the system
is binomially distributed, $C^k_t = \sum_{i=1}^{m_k} \indic(T^k_i > t) \sim \bin(\ptk, m_k)$.
%(maybe not relevant for this manuscript, but would it make sense to take Kaplan-Meier estimates for the $p^k_t$'s,
%or the NPI bounds for Kaplan-Meier?)

The $\ptk$'s can, in theory, be directly chosen to arbitrarily closely approximate any valid lifetime pdf on
$[0,\infty)$, for example matching a bathtub curve for the corresponding hazard rate $h^k(t_j)$.
Naturally, $p^k_{t_j} \ge p^k_{t_{j+1}}$ should hold (assuming no repair).
However, such direct specification is non-trivial, neglects any inherent uncertainty in the particular choice, and cannot be 
easily combined with test data.
To account for the uncertainty, one can express knowledge about $\ptk$ through a prior distribution.
A convenient and natural choice is $\ptk \sim \be(\alpha^k_t, \beta^k_t)$, particularly because in a Bayesian inferential
setting this is the conjugate prior which leads to a Beta posterior.

Let the lifetime test data collected on component $k$ be $\vec{t}^k = (t^k_1, \ldots, t^k_{n_k})$.
At each fixed time $t \in {\cal T}$, this corresponds to an observation from the Binomial model described above,
$s^k_t = \sum_{i=1}^{n_k} \indic(t^k_i > t)$.
The posterior is then $\ptk \mid s^k_t \sim \be(\alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)$.
The combination of a Binomial observation model with a Beta prior is often called Beta-Binomial model.

The posterior predictive distribution for the number of components surviving at time $t$
in a new system, based on the lifetime data and the prior information,
is then a so-called Beta-Binomial distribution,
$C^k_t \mid s^k_t \sim \bebin(m_k, \alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)$.
That is, we have
\begin{align*}
P(C^k_t = l_k \mid s^k_t) &= {m_k \choose l_k} \frac{B(l_k + \alpha^k_t + s^k_t, m_k - l_k + \beta^k_t + n_k - s^k_t)}
                                                    {B(\alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)} \,,
\end{align*}
where $B(\cdot, \cdot)$ is the Beta function.
This posterior predictive distribution is essentially a Binomial distribution
where the functioning probability $\ptk$ takes values in $[0,1]$
with the probability given by the posterior on $\ptk$.

Consequently, in order to calculate the system reliability according to \eqref{eq:rsyswithsurvsign},
for each component type $k$
we need lifetime data $\vec{t}^k$,
and have to choose $2 \times |{\cal T}|$ parameters
to specify the prior distribution for the discrete survival function of $T^k_i$.
In total, values for $2 \times |{\cal T}| \times K$ parameters must be chosen.


\section{Reparametrisation of the Beta Distribution}
\label{sec:reparam}

The parametrisation of the Beta distribution used above is common,
and allows $\alpha^k_t$ and $\beta^k_t$ to be interpreted as
hypothetical numbers of functioning and failed components of type $k$ at time $t$, respectively.
However, when we generalise to sets of priors in the sequel,
it is useful to consider a different parametrisation.

For clarity of presentation we will temporarily drop the super- and subscript $k$ and $t$ indices for component type and time.
Instead of $\alpha$ and $\beta$, we consider the parameters $\nz \in [0, \infty)$ and $\yz \in [0,1]$, where
\begin{align}
\nz &= \alpha + \beta &
&\text{and} &
\yz &= \frac{\alpha}{\alpha + \beta} \,,
\label{eq:reparam}
\end{align}
or equivalently, $\alpha = \nz\yz$ and $\beta = \nz(1-\yz)$.
The upper index ${}\uz$ is used to identify these as prior parameter values,
in contrast to their posterior values $\nn$ and $\yn$
obtained after observing $n$ failure times (see below).
$\nz$ and $\yz$ are sometimes called \emph{canonical} parameters,
identified from rewriting the density in canonical form;
see for example \cite[pp.~202 and 272f]{2000:bernardosmith}, or \cite[\S 1.2.3.1]{2013:diss-gw}.
This canonical form gives a common structure to all conjugacy results in exponential families.

From the properties of the Beta distribution,
it follows that $\yz = \E[p]$ is the prior expectation for the functioning probability $p$,
and that larger $\nz$ values lead to greater concentration of probability measure around $\yz$,
since $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.
Consequently, $\nz$ represents the prior strength and moreover can be directly interpreted as a pseudocount, as will become clear.
%this is clear from the interpretation of $\alpha$ and $\beta$ and \eqref{eq:reparam},
%and will be further illustrated below through revisiting the posterior
%%based on observed lifetimes $\vec{t} = (t_1, \ldots, t_{n})$.
Indeed, consider the posterior given that $s$ out of $n$ components function:
by conjugacy
$p \mid s$ is Beta distributed with updated parameters
\begin{align}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,.
\label{eq:nyupdate}
\end{align}
Thus, after observing that $s$ out of $n$ components function (at time $t$),
the posterior mean $\yn$ for $p$ is a weighted average of
the prior mean $\yz$ and $s/n$ (the fraction of functioning components in the data),
with weights $\nz$ and $n$, respectively.
Therefore $\nz$ takes on the same role for the prior mean $\yz$
as the sample size $n$ does for the observed mean $s/n$,
leading to the notion of it being a pseudocount.
%Indeed, the higher $\nz$, the higher the weight for $\yz$
%in the weighted average calculation of $\yn$,
%so $\nz$ gives the strength of the prior as compared to the sample size $n$.
%The parameters $\nz$ and $\yz$ have a more intuitive interprtation,

Reintroducing time and component type indices, the posterior predictive Beta-Binomial probability mass function (pmf) can be written in terms of the updated parameters as
\begin{align}
P(C^k_t = l_k \mid s^k_t) &= {m_k \choose l_k} \frac{B(l_k + \nn_{k,t}\yn_{k,t}, m_k - l_k + \nn_{k,t}(1-\yn_{k,t}))}
                                                    {B(\nn_{k,t}\yn_{k,t}, \nn_{k,t}(1-\yn_{k,t}))} \,,
\label{eq:postpredCny}
\end{align}
with the corresponding cumulative mass function (cmf) given by
\begin{align}
F_{C^k_t\mid s^k_t}(l_k) &= P(C^k_t \le l_k \mid s^k_t) = \sum_{j_k=0}^{l_k} P(C^k_t = j_k \mid s^k_t)\,.
\label{eq:postpredCnycmf}
\end{align}


The parameterisation in terms of prior mean and prior strength (or pseudocount)
makes clear that in this conjugate setting,
learning from data corresponds to averaging between prior and data.
This form is attractive not only because it enhances the interpretability of the model and prior specification,
but crucially it also makes clear what should be a serious concern in any Bayesian analysis:
when observed data differ greatly from what is expressed in the prior,
this conflict is simply averaged out
and is not reflected in the posterior or posterior predictive distributions.

As a simple example, imagine that we expect $\ptk$ to be about $0.75$ for a certain $k$ and $t$,
so we choose $\yktz = 0.75$,
and that we value this choice of mean functioning probability with $\nktz = 8$,
i.e., equivalently to having seen $8$ observations with a mean $0.75$.
If we observe $n_k = 16$ components of type $k$ in the test data and $s^k_t = 12$ function at time $t$,
then $s^k_t/n_k = 0.75$ as we expect,
so that the updated parameters are $\nktn = 24, \yktn=0.75$.
However, in contrast, unexpectedly observing that no component functions at time $t$ instead
leads to parameters $\nktn = 24, \yktn=0.25$.
The prior and the posteriors based on these two scenarios
are depicted in the left panels of Figure~\ref{fig:singleprior-pdc},
%through their Beta densities and cdfs (left)
%and corresponding Beta-binomial probabilities $P(C^k_t = l_k\mid s^k_t)$ and cmfs
along with their corresponding predictive Beta-binomial pmf and cmf
for the case $m_k = 5$ (right panels).

Due to symmetry, we see that both posteriors have the same variance,
although arising from two fundamentally different scenarios.
Posterior 1 is based on data exactly according to prior expectations;
the increase in confidence on $\ptk \approx 0.75$
is reflected in a more concentrated posterior density,
and the posterior predictive is changed only slightly.
However, it may be cause for concern to see the same degree of confidence in Posterior 2,
which is based on data that is in sharp conflict with prior expectations.
Posterior 2 places most probability weight around $0.25$,
averaging between prior expectation and data,
with the same variance as Posterior 1.
Accordingly, rather than conveying the conflict between observed
and expected functioning probabilities with increased variance,
Posterior 2 instead gives a false sense of certainty.

\begin{figure}
\includegraphics[width=\textwidth]{singleprior-pdc2}
\caption{Beta densities (top left) and cdfs (bottom left),
with the corresponding Beta-binomial predictive probability mass functions (top right) and cumulative mass functions (bottom right),
for a prior with $\nktz = 8, \yktz = 0.75$,
and posteriors based on $n^k_t=16$ observations with $s^k_t=12$ (Posterior~1) and $s^k_t=0$ (Posterior~2), respectively.
Data for Posterior~1 confirm prior assumptions,
while data for Posterior~2 are in conflict with the prior.
However, this conflict is averaged out,
and Posterior 1 and Posterior 2 have the same spread, both in the posterior pdf/cdf and the posterior predictive pmf/cmf,
such that Posterior 2 gives a false sense of certainty despite the massive conflict between prior and data.}
\label{fig:singleprior-pdc}
\end{figure}

To enable diagnosis of when this undesirable behaviour occurs,
we propose to use an imprecise probability approach
based on sets of Beta priors, described in the following section.


\section{Sets of Beta Priors}
\label{sec:setsofbetapriors}

As was shown by \citet{2009:WalterAugustin}, %Walter \& Augustin (2009),
we can have both tractability and meaningful reaction to prior-data conflict
by using sets of priors $\MktZ$ produced by parameter sets $\PktZ = [\nktzl, \nktzu] \times [\yktzl, \yktzu]$
(a detailed discussion of different choices for $\PktZ$ is given in \citet[\S 3.1]{2013:diss-gw}.)
In our model, each prior parameter pair $(\nktz, \yktz) \in \PktZ$
corresponds to a Beta prior, thus $\MktZ$ is a set of Beta priors.
The set of posteriors $\MktN$ is obtained by updating each prior in $\MktZ$ according to Bayes' Rule.
This element-by-element updating can be rigorously justified
as ensuring coherence \citep[\S 2.5]{1991:walley}, and was termed ``Generalized Bayes' Rule'' by \citet[\S 6.4]{1991:walley}.
Due to conjugacy, $\MktN$ is a set of Beta distributions with parameters $(\nktn, \yktn)$,
obtained by updating $(\nktz, \yktz) \in \PktZ$ according to \eqref{eq:nyupdate},
leading to the set of updated parameters
\begin{align}
\PktN &= \Big\{ (\nktn, \yktn) \mid (\nktz, \yktz) \in \PktZ = [\nktzl, \nktzu] \times [\yktzl, \yktzu] \Big\}\,.
\label{eq:paramsets}
\end{align}

Examples for parameter sets $\PktZ$ and $\PktN$ as in \eqref{eq:paramsets} are depicted in Figure~\ref{fig:paramsets}.
Such rectangular prior parameter sets $\PktZ$ have been shown
to balance desirable model properties and ease of elicitation 
(see \citet[pp.~123f]{2013:diss-gw} or \citet{Troffaes2013a-short}).
%
For each component type $k$ and time point $t$,
one need only specify the four parameters $\nktzl, \nktzu, \yktzl, \yktzu$
(so in total $4 \times |{\cal T}|$ parameters are needed to define the set of prior distributions
on the survival function of each component).
%notwithstanding this, the model has favourable inference properties.

\begin{figure}
\includegraphics[width=\textwidth]{paramsets}
\caption{Prior parameter set $\PktZ = [1,8] \times [0.7,0.8]$ and posterior parameter set $\PktN$
for data $s^k_t/n_k = 12/16$ (Posterior 1, left) and $s^k_t/n_k = 0/16$ (Posterior 2, right).
For no-conflict data ($s^k_t/n_k \in [\yktzl,\yktzu]$), $\PktN$ has the `spotlight' shape (left);
in case of prior-data conflict ($s^k_t/n_k \not\in [\yktzl,\yktzu]$), $\PktN$ has the `banana' shape (right),
leading to a large degree of imprecision in the $\yktn$ dimension of $\PktN$,
thus reflecting increased uncertainty about the functioning probability $p^k_t$
due to the conflict between prior assumptions and observed data.}
\label{fig:paramsets}
\end{figure}

A desirable inference property arising from this setup is that
the posterior parameter set $\PktN$ is not rectangular in the way that the prior parameter set is.
Indeed, the shape of $\PktN$ depends on the presence or absence of prior-data conflict,
which is naturally operationalised as $s^k_t/n_k \not\in [\yktzl, \yktzu]$:
that is, prior-data conflict is defined to occur when, at time $t$, the observed fraction of functioning components
is outside its \emph{a priori} expected range.

First, in the absence of prior-data conflict, 
$\PktN$ shrinks in the $\ykt$ dimension;
how much it shrinks depending on $\nktz \in [\nktzl, \nktzu]$,
leading to the so-called spotlight shape depicted in Figure~\ref{fig:paramsets} (left).
Since $\yktn$ gives the posterior expectation for the functioning probability $p_t^k$,
shorter $\yktn$ intervals mean more precise knowledge about $p_t^k$.
Also, the variance interval for $p_t^k$ (not shown) will shorten and shift towards zero,
as the Beta distributions in $\MktN$ will be more concentrated
due to the increase of $\nktz$ to $\nktn$.

Alternatively, when there is conflict between prior and observed data (i.e.\ $s^k_t/n_k \not\in [\yktzl, \yktzu]$),
$\PktN$ instead adopts the so-called `banana shape',
arising from the intervals for $\yktn$  being shifted closer to $s^k_t/n_k$
for lower $\nktn$ values than for higher $\nktn$ values, see Figure~\ref{fig:paramsets} (right).
Overall, this results in a wider $\yktn$ interval compared to the no conflict case, 
reflecting the extra uncertainty due to prior-data conflict.  
In other words, the posterior sets make more cautious probability statements about $p_t^k$, as desired in this scenario.

Based on these shapes and \eqref{eq:nyupdate}, it is possible to deduce the following expressions
for the lower and upper bounds of $\yktn$:
\begin{align}
\begin{aligned}
\min_{\PktN} \yktn &=
 \begin{cases}
 %\dfrac{\nktzu \yktzl + s^k_t}{\nktzu + n_k} & \text{if } s^k_t/n_k \ge \yktzl \\
 \big(\nktzu \yktzl + s^k_t\big) / \big(\nktzu + n_k\big) & \text{if } s^k_t/n_k \ge \yktzl \\
 %\dfrac{\nktzl \yktzl + s^k_t}{\nktzl + n_k} & \text{if } s^k_t/n_k <   \yktzl
 \big(\nktzl \yktzl + s^k_t\big) / \big(\nktzl + n_k\big) & \text{if } s^k_t/n_k <   \yktzl
 \end{cases}\,,\\
\max_{\PktN} \yktn &=
 \begin{cases}
 %\dfrac{\nktzu \yktzu + s^k_t}{\nktzu + n_k} & \text{if } s^k_t/n_k \le \yktzu \\
 \big(\nktzu \yktzu + s^k_t\big) / \big(\nktzu + n_k\big) & \text{if } s^k_t/n_k \le \yktzu \\
 %\dfrac{\nktzl \yktzu + s^k_t}{\nktzl + n_k} & \text{if } s^k_t/n_k >   \yktzu
 \big(\nktzl \yktzu + s^k_t\big) / \big(\nktzl + n_k\big) & \text{if } s^k_t/n_k >   \yktzu
 \end{cases}\,.
\end{aligned}
\label{eq:ysetupdate}
\end{align}
Note that the lower bound for $\yktn$ is always attained at $\yktzl$, the upper bound at $\yktzu$.
Also note that when $s^k_t/n_k \in [\yktzl, \yktzu]$,
both the lower and the upper bounds for $\yktn$ are attained at $\nktzu$,
corresponding to the spotlight shape.
However, when $s^k_t/n_k \not\in [\yktzl, \yktzu]$,
the banana shape indicates that
one of the bounds for $\yktn$ is attained at $\nktzl$.

\begin{figure}
\includegraphics[width=\textwidth]{betaset-binomset1}
\caption{Sets of Beta pdfs (left) and Beta-Binomial cmfs (right, for $m_k=5$)
corresponding to the prior and posterior parameter sets in Figure~\ref{fig:paramsets}.
The sets are depicted as shaded areas,
with the distributions corresponding to the four corners
of the prior parameter set $\PktZ$ (or their posterior counterparts) as solid lines.
The top row depicts the set of prior cdfs/cmfs and the set of posterior cdfs/cmfs for the case where data confirm prior assumptions
(see left panel of Figure~\ref{fig:paramsets});
the bottom row depicts the (identical) set of prior cdfs/cmfs and the set of posterior cdfs/cmfs in case of prior-data conflict
(see right panel of Figure~\ref{fig:paramsets}).
%The set of posteriors and the set of posterior predictive distributions
The set of posterior cdfs and cmfs
is much larger in case of prior-data conflict:
uncertainty due to this conflict is reflected through increased imprecision.}
\label{fig:betaset-binomset}
\end{figure}

The different locations and sizes of $\PktN$ in the conflict versus no conflict case are then, in turn,
also reflected in the corresponding sets of Beta cdfs and Beta-Binomial cmfs.
As an example, those corresponding to the parameter sets in Figure~\ref{fig:paramsets}
are depicted in Figure~\ref{fig:betaset-binomset}.

In the no conflict case (Posterior 1, top row),
the reduction of the $\yktn$ range in $\PktN$ leads to a much smaller set
of Beta and Beta-Binomial distributions.
For example, the range of predictive probabilities
that two out of a set of five components of type $k$ function at time $t$ 
has changed from $[0.10, 0.28]$ \emph{a priori} to $[0.11,0.14]$ \emph{a posteriori}.
This reflects the gain in precision due to test data in accordance with prior assumptions.

In contrast, for the prior-data conflict case (Posterior 2, bottom row),
the wide $\yktn$ range in $\PktN$ leads to a set of Beta and Beta-Binomial distributions
that is much larger than in the no conflict case.
Here, the range of posterior predictive probabilities
that two out of a set of five components of type $k$ function at time $t$
is now $[0.86, 1.00]$ \emph{a posteriori}, i.e., less precise than in the no conflict case.
Using sets of Beta priors, the resulting set of posterior predictive Beta-Binomial distributions
reflects the precision of prior information,
the amount of data, and prior-data conflict.

Furthermore, with sets of Beta priors it is also possible
to express prior ignorance
by letting $\yktzl \to 0$ and $\yktzu \to 1$
for some or all $t \in {\cal T}$.
(Note that it is not advisable to choose $\yktzl = 0$ and $\yktzu = 1$,
as this can lead to improper posterior predictive distributions.
For example, at any $t < \min(\vec{t}^k)$,
we would have $\yktnu = 1$, leading to one argument of the Beta function
in the denominator of \eqref{eq:postpredCny} being zero.)
These limits for $\yktz$ imply we are only prepared to give trivial bounds for the functioning probability
and do not wish to commit to any specific knowledge about $p^k_t$ \emph{a priori}.
This provides a more natural choice of `noninformative' prior over $[0,1]$ than the usual choice of
a Beta prior with $\alpha^k_t = \beta^k_t = 1$ (or $\nktz = 2$, $\yktz = 0.5$).
Such a prior for all $t \in {\cal T}$ actually reflects a belief that the component reliability function
is on average 1/2 for all $t$, which is not an expression of ignorance, but rather a very specific (and arguably peculiar) prior belief.

%With the bounded domain for $\yktz$ 
%\td{***called near-noninformative, for near-noninformative sets when $\yz$ is not bounded see
%benavoli zaffalon papers***}
%
%As a note regarding the bounds for $\nktz$,
%for an informative prior the choice of $\nktzl$ is relevant for posterior inferences in case of prior-data conflict,
%as then one of the bounds for $\ykt$ is attained for $\nktzl$.
%In a near-noninformative setting, the choice of $\nktzl$ is instead not relevant,
In a near-noninformative setting, the choice of $\nktzl$ is not relevant,
because \eqref{eq:ysetupdate} implies both lower and upper bound for $\yktn$ are obtained with $\nktzu$.
In particular, $\yktzl > 0$ and $\yktzu < 1$ can be chosen such that
$\frac{s^k_t}{n_k} \in [\yktzl, \yktzu]$ for all $t \in \big(\min(\vec{t}^k), \max(\vec{t}^k)\big)$.
Naturally, one cannot have prior-data conflict in cases of near prior ignorance.


\section{Sets of System Reliability Functions}
\label{sec:setsofrel}

The elements reviewed and extended above culminate hereinafter in the primary contribution of the current work,
providing a framework in which the nonparametric Bayesian system reliability approach developed in \cite{2015:bayessurvsign}
is extended to incorporate the sets of priors approach of \citet{2009:WalterAugustin},
enabling diagnoses of prior-data conflict which is consequential at the system
level, via sets of system reliability functions.

To obtain the lower and upper bound for the system reliability function $\Rsys(t)$,
we now need to minimise and maximise Equation~\eqref{eq:rsyswithsurvsign} over $\PtZi{1}, \ldots, \PtZi{K}$ for each $t$,
where the posterior predictive probabilities for $C^k_t$ are given by the Beta-Binomial pmf \eqref{eq:postpredCny}.
%It turns out that we need to optimise over $\ntz{1}, \ldots, \ntz{K}$ only, %$\nktz$ ($k=1,\ldots,K$) only,
%as lower and upper bounds for $\Rsys(t)$ are obtained for $\yktzl$ and $\yktzu$, respectively.
%
%To see this,
%remember that $\yktz$ gives the prior expected functioning probability for a component of type $k$ at time $t$.
%From Equation~\eqref{eq:nyupdate} (right) we see that $\yktn$ is increasing in $\yktz$,
%%the posterior parameter set bounds are the transformed prior parameter set bounds,
%%i.e., $\yktzu$ corresponds to the upper bound of $\PktN$, for all $\nktz \in \PktZ$,
%such that for each fixed $\nktz$, $\yktzu$ will maximise the posterior expected functioning probability $\yktn$.
%%
%Now, a higher posterior expected functioning probability means
%higher probability for many components out of $m_k$ surviving,
%i.e., probabilities for the larger $l_k$'s are higher, and in turn for the lower $l_k$'s are lower.
%Indeed, the Beta-Binomial distribution is stochastically ordered for varying $\yktn$, i.e.,
%its cmf \eqref{eq:postpredCnycmf} is decreasing in $\yktn$ for all $l_k$,
%such that the cmfs for different values of $\yktn$ do not cross each other.
%This means that for each fixed $\nktz$, the Beta-Binomial cmf \eqref{eq:postpredCnycmf} is minimized for $\yktzu$ in all $l_k$,
%such that the probability weights given to high values of $l_k$ are maximised. %for $\yktzu$.
%For coherent systems, $\Phi$ is non-decreasing in each of its arguments $l_1,\ldots,l_K$,
%thus $\Rsys(t)$ is maximised by putting most probability weight on the highest $l_k$ values for each $k$,
%which is in turn obtained for $\yktzu$, $k=1,\ldots,K$.
We therefore have
%
%for a certain $k$, a high probability on high $l_k$'s is never worse than a high probability on lower $l_k$'s,
%keeping all other component type expressions fixed.
%***Monotonicity in $\yz_{k,t}$, so we get, for each $t \in {\cal T}$,
\begin{align}
\lefteqn{\lRsys(t \mid \vec{t}^1, \ldots, \vec{t}^K)} \nonumber \\
 &= \min_{\PtZi{1}, \ldots, \PtZi{K}} \Rsys(t \mid \PtZi{1}, \ldots, \PtZi{K}, \vec{t}^1, \ldots, \vec{t}^K) \nonumber \\
 &= \min_{\PtZi{1}, \ldots, \PtZi{K}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K)
                                                 \prod_{k=1}^K P(C^k_t = l_k \mid \yktz, \nktz, s^k_t) \nonumber \\
 &= \min_{\PtZi{1}, \ldots, \PtZi{K}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K) \times \nonumber \\ & \hspace*{12ex}
    \prod_{k=1}^K {m_k \choose l_k} \frac{B(l_k + \nn_{k,t}\yn_{k,t}, m_k - l_k + \nn_{k,t}(1-\yn_{k,t}))}
                                         {B(\nn_{k,t}\yn_{k,t}, \nn_{k,t}(1-\yn_{k,t}))} \nonumber \\
 &= \min_{\PtZi{1}, \ldots, \PtZi{K}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K) \times \nonumber \\ & \hspace*{8ex}
    \prod_{k=1}^K {m_k \choose l_k} \frac{B(l_k + \nktz\yktz + s^k_t, m_k - l_k + \nktz(1-\yktz) + n_k - s^k_t)}{B(\nktz\yktz + s^k_t, \nktz(1-\yktz) + n_k - s^k_t)}
    \,, \label{eq:LwrSysPostA}
\end{align}
and similarly maximising for $\uRsys(\cdot)$.

Note that $\Phi(\cdot)$ is non-decreasing in each of its arguments $l_1,\ldots,l_K$,
thus if there is first-order stochastic ordering on $P(C^k_t = l_k \mid \yktz, \nktz, s^k_t)$
for each $k$, then this ordering can be used to determine the elements of $\PtZi{k}$
which minimise and maximise the overall system reliability function without
resorting to computationally expensive exhaustive searches or numerical optimisation.

We therefore start by proving the following result, where indices are suppressed for readability.
We use $\ge_{\mathrm{st}}$ to denote first-order stochastic dominance.

\begin{theorem}
  \label{thm:y}
  Let $\beta_y$ denote the Beta-Binomial distribution with probability mass function parameterised as:
  \[ p(l \mid y, n, m, s, N) \propto \frac{B(l + ny + s, m - l + n(1-y) + N - s)}{B(ny + s, n(1-y) + N - s)}, \]
  with $n, m, s,$ and $N$ fixed and unknown.
  
  Then $\beta_{\yu} \ge_{\mathrm{st}} \beta_{\yl} \ \forall \ \yu > \yl$ with $\yu, \yl \in (0,1)$.
\end{theorem}
The proof is provided in Appendix \ref{ap:proofs}, p.\pageref{prf:y}.

Consequently, for each component, the posterior predictive Beta Binomial distributions 
with larger prior functioning probability stochastically dominate those
with smaller prior functioning probability, providing rigorous proof 
which accords with intuition. Applying this result to the sets of system reliability 
functions, together with the monotonicity in the survival signature, means that
$\lRsys(\cdot)$ is attained when $\yktz = \yktzl$ and $\uRsys(\cdot)$
is attained when $\yktz = \yktzu$ for all possible $\nktz$ values.

The analogous result for $\nktz$ is more subtle, because stochastic 
dominance is not guaranteed at a single value.  The following Theorem 
provides the conditions under which an upper or lower limit has first-order stochastic dominance.

\begin{theorem}
  \label{thm:n}
  Let $\beta_n$ denote the Beta-Binomial distribution with probability mass function parameterised as:
  \[ p(l \mid y, n, m, s, N) \propto \frac{B(l + ny + s, m - l + n(1-y) + N - s)}{B(ny + s, n(1-y) + N - s)}, \]
  with $y, m, s,$ and $N$ fixed and unknown.  
  Then,
  \[ y > \frac{s + m}{N + m - 1} \implies \beta_{\nu} \ge_{\mathrm{st}} \beta_{\nl} \]
  and 
  \[ y < \frac{s}{N + m - 1} \implies \beta_{\nu} \le_{\mathrm{st}} \beta_{\nl} \]
  If $\frac{s}{N + m - 1} < y < \frac{s + m}{N + m - 1}$ then first-order stochastic dominance is not guaranteed.
\end{theorem}
The proof is provided in Appendix \ref{ap:proofs}, p.\pageref{prf:n}.

These first-order stochastic domination results now gives the exact 
solution for $\lRsys(\cdot)$ and $\uRsys(\cdot)$.  Thus,
%For $\lRsys(\cdot)$, the minimum is achieved at
%$\nktzu$ if $\yktzl(n_k+m_k-1)-s^k_t-m_k < 0$ and is achieved at 
%$\nktzl$ if $\yktzl(n_k+m_k-1)-s^k_t-m_k > 0$.
%Conversely, for $\uRsys(\cdot)$, the maximum is achieved at 
%$\nktzu$ if $\yktzu(n_k+m_k-1)-s^k_t-m_k > 0$ and is achieved at 
%$\nktzl$ if $\yktzu(n_k+m_k-1)-s^k_t-m_k < 0$.
\begin{align}
\lefteqn{\lRsys(t \mid \vec{t}^1, \ldots, \vec{t}^K)} \nonumber \\
 &= \min_{\PtZi{1}, \ldots, \PtZi{K}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K) \times \nonumber \\ & \hspace*{8ex}
    \prod_{k=1}^K {m_k \choose l_k} \frac{B(l_k + \nktz\yktz + s^k_t, m_k - l_k + \nktz(1-\yktz) + n_k - s^k_t)}{B(\nktz\yktz + s^k_t, \nktz(1-\yktz) + n_k - s^k_t)}
    \nonumber\\
 &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K) \times \nonumber \\ & \hspace*{8ex}
    \prod_{k=1}^K {m_k \choose l_k} \frac{B(l_k + \nktzo\yktzl + s^k_t, m_k - l_k + \nktzo(1-\yktzl) + n_k - s^k_t)}{B(\nktzo\yktzl + s^k_t, \nktzo(1-\yktzl) + n_k - s^k_t)} \label{eq:LwrSysPost}\\
 &\qquad \qquad \mbox{where } \nktzo = \left\{ \begin{aligned}
   \nktzu & \quad\mbox{if}\quad \yktzl < \frac{s}{N + m - 1} \\
   \nktzl & \quad\mbox{if}\quad \yktzl > \frac{s + m}{N + m - 1} \\
   \mathrm{optimised} & \quad\mbox{if}\quad \frac{s}{N + m - 1} < \yktzl < \frac{s + m}{N + m - 1}
 \end{aligned} \right. \nonumber
\end{align}

\td{*** Louis: I've edited to here so far, will continue editing rest of this section if we are happy with the above! ***}

In the following, we will give some guidelines on how to choose the parameter sets $\PkZi{1}, \ldots, \PkZi{\tmax}$
which define the set of prior discrete reliability functions for components of type $k$.
We think this is much easier in terms of $\nz$ and $\yz$ than it would be in terms of $\alpha$ and $\beta$.
%***How to choose the sets of priors for the survival function of type $k$ components,
%i.e. how to choose $\PkZi{1}, \ldots \PkZi{\tmax}$?***

As mentioned in Section~\ref{sec:nonparamapproach}, for the actual functioning probabilities $\ptk$
it should hold that $p^k_{t_j} \ge p^k_{t_{j+1}}$. (***or $p^k_{t_i} \ge p^k_{t_i}$ for all $i < j$).
%***not necessary to ensure $p^k_{t_j} \ge p^k_{t_{j+1}}$ via $\yzl_{k,t_j} \ge \yzu_{k,t_{j+1}}$
%as for relatively dense $t$ grids, neighbouring intervals should be able to overlap***
%(Prior guess for $p^k_t$ interpretation would say that $\yktz$ cannot increase,
%but does this need to hold for the prior guess range bound $\yktzu$?)***
This naturally translates to conditions on the prior guess for $\ptk$,
such that we propose that 
$\yzu_{k,t_j} \ge \yzu_{k,t_{j+1}}$ and $\yzl_{k,t_j} \ge \yzl_{k,t_{j+1}}$ should hold.
Due to $s^k_t/n_k$ decrasing in $t$ and the weighted average property of the update step for $\ykt$,
this ensures that 
$\ynu_{k,t_j} \ge \ynu_{k,t_{j+1}}$ and $\ynl_{k,t_j} \ge \ynl_{k,t_{j+1}}$.
In a situation where we are quite sure about the functioning probability for low $t$,
but are unsure about what happens for larger $t$ and so want to be maximally cautious for those $t$,
then we can let $\yktzl$ drop to (almost) 0, but $\yktzu$ should not increase.
%***Require $\yzu_{k,t_j} \ge \yzu_{k,t_{j+1}}$ and $\yzl_{k,t_j} \ge \yzl_{k,t_{j+1}}$.
%If we know, e.g., pretty much what's going on for low $t$
%but want to be much more cautious for high $t$,
%then $\yktzl$ can drop to 0, but $\yktzu$ cannot increase.

We think it is not advisable to express certainty about expected functioning probabilities
with $\nktz$ bounds that vary strongly over $t$.
%***Better to express certainty of prior guess via $\nktz$ bounds?\\
With (strongly) differing $\nktz$ bounds, monotonicty for $\yktn$ bounds cannot be ensured.
%it can happen that, e.g., $\ynu_{k,t_j} < \ynu_{k,t_{j+1}}$!\\
E.g., when $\yzu_{k,t_j} = \yzu_{k,t_{j+1}}$, $\yzl_{k,t_j} = \yzl_{k,t_{j+1}}$,
and $s^k_{t_j}/n_k \in [\yzl_{k,t_j}, \yzu_{k,t_j}]$ (so there is no prior-data conflict),
and furthermore no failures happening in $[t_j, t_{j+1}]$, i.e., $s^k_{t_{j+1}}/n_k = s^k_{t_{j}}/n_k$, then
\begin{itemize}
\item for $\nzu_{k,t_j} < \nzu_{k,t_{j+1}}$ we get $\ynu_{k,t_j} < \ynu_{k,t_{j+1}}$, and
\item for $\nzu_{k,t_j} > \nzu_{k,t_{j+1}}$ we get $\ynl_{k,t_j} < \ynl_{k,t_{j+1}}$.
\end{itemize}
Again, this follows from the weighted average property of $\yktn$;
for constant $\yktz$, the lower $\nktz$ the closer $\yktn$ is to $s^k_t/n_k$.
% one can think of the update between interval and precise value: the lower $n$, the 'faster' the interval will shrink!)\\
It is possible to construct similar examples also with regard to the lower bound $\nzl_{k,t_j}$.
Therefore, we propose to take the same $\nktz$ bounds for all $t$, or, at most, to let them change with $t$ gradually only.

Generally, the interpretation as pseudocount or prior strength should guide the choice of bounds for $\nktz$; 
low values for $\nktz$ as compared to the test sample size $n_k$
give low weight to the prior expected functioning probability intervals $[\yktzl, \yktzu]$,
and the location of posterior intervals $[\yktnl, \yktnu]$ will be dominated by the location of $s^k_t/n_k$.
Furthermore, the length of $[\yktnl, \yktnu]$ is shorter for low $\nktz$ values;
in a no-conflict situation, when $\nktzu = n_k$ then $[\yktnl, \yktnu]$ has half the length of $[\yktzl, \yktzu]$.
In contrast, high values for $\nktz$ will lead to slower learning and wider $\yktn$ intervals,
which means more cautious posterior inferences.
The difference betweeen $\nktzu$ and $\nktzl$ determines the strength of the prior-data conflict sensitivity;
as is clear from Figure~\ref{fig:paramsets} and \eqref{eq:ysetupdate}, the wider the $\nktz$ interval,
the wider $[\yktnl, \yktnu]$ in case of conflict.
So it seems useful to choose $\nktzl = 1$ or $\nktzl = 2$,
while choosing $\nktzu$ with help of the half-width rule as described above.

%***generally require $\yktzl > 0$ and $\yktzu < 1$ for all $t \not\in \big(\min(\vec{t}^k), \max(\vec{t}^k)\big)$
%to get proper posteriors***
As mentioned in Section~\ref{sec:setsofbetapriors},
it is not advisable to choose $\yktzl = 0$ and $\yktzu = 1$.
For any $t \not\in \big(\min(\vec{t}^k), \max(\vec{t}^k)\big)$,
this can lead to improper posterior predictive distributions
due to zero arguments for the Beta functions in the denominator of \eqref{eq:LwrSysPost}.
%Generally, one should choose $\yktzl > 0$ and $\yktzu < 1$ for all $t \not\in \big(\min(\vec{t}^k), \max(\vec{t}^k)\big)$.
But it is possible to choose values close to $0$ and $1$, respectively,
and due to the linear update step for $\yktn$ (see Eq.~\eqref{eq:nyupdate}),
posterior inferences are not overly sensitive to whether $\yktnu = 0.99$ or $\yktnu = 0.9999$.
%***what to do in extreme tail of component distributions?
%How sensitive is the method to choosing prior (lower, upper) mean near-zero (how near?) for $p^k_t$ at high $t$'s?***
Likewise, our nonparametric method does not bring unintuitive tail behaviour
as some parametric methods do;
there is no problem in, e.g., assigning $\yktnu$ near-zero for high $t$ if prior knowledge suggests so.

%***elicitation: we should also consider the case where $\yktz$ bounds are not given for all $t \in {\cal T}$,
%as it will be too time-consuming to elicit $\yktz$ bounds for all $t$ in a dense grid.
While it is possible to set the bounds $\yktzl$ and $\yktzu$ for each $t \in {\cal T}$ individually,
in practice this will be often too time-consuming when ${\cal T}$ forms a dense grid.
%(Calculating $\Rsys(t)$ for a too coarse grid only will waste information from data.)
Switching to a coarser time grid will waste information from data,
as then failure times in the test data are rounded up to the next $t \in {\cal T}$.
We rather propose to elicit bounds for a subset of ${\cal T}$ only,
and to fill up the time grid with the least committal bounds, i.e.,
taking $\yktzu$ equal to last (in the time sequence) elicited $\yktzu$,
and likewise $\yktzl$ equal to next (in time sequence) elicited $\yktzl$.
A possible elicitation procedure in this vein could be
to start with eliciting $\yktz$ bounds for a few `central' time points $t$,
filling up the grid as described above accordingly,
and then to further refine the obtained bounds as deemed necessary by the expert.
%`middle' or `typical' $t$,
%then $\yktz$ bounds for $t$ smaller than `middle' $t$ (or halfway between 0 and `middle' $t$?) and for $t$ larger than `middle' $t$,


\section{Computations \& Examples}

%***code published in Louis' \textbf{R} package \texttt{ReliabilityTheory},

\subsection{Software}

The methods of this paper have been implemented in the \textbf{R}
\citep{R} package \texttt{ReliabilityTheory} \citep{2015:aslett-RT},
providing an easy to use interface for reliability practitioners.  The
primary function, which computes the upper and lower posterior
predictive system survival probabilities as in \eqref{eq:LwrSysPost}, is
named \texttt{nonParBayesSystemInferencePriorSets()}.  The user
specifies the times at which to evaluate the bounds, the survival
signature ($\Phi(\cdot)$), the component test data ($\vec{t}^1, \ldots,
\vec{t}^K$), and the prior parameter set for each component type and
time ($\PtZi{k}$, via $\nzu_{k,t}, \nzl_{k,t}, \yzu_{k,t}$, and
$\yzl_{k,t}$).  All computations of $\lRsys$
and $\uRsys$ at different time points are performed in parallel
automatically where the CPU has multiple cores.

Note that computation of the system signature itself can be simplified by
expressing the structure of the system as an undirected graph using the
\texttt{computeSystemSurvivalSignature()} function in the same package,
leaving only data and prior to be handled.  These publicly available
functions have been used in computing all the following examples for
reproducibility.  \ref{ap:software} details how to use this software.

\subsection{Examples}

\begin{figure}
\centering
\begin{tikzpicture}
[type1/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 type2/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 type3/.style={rectangle,draw,fill=black!20,very thick,inner sep=0pt,minimum size=8mm},
 cross/.style={cross out,draw=red,very thick,minimum width=7mm, minimum height=5mm},
 hv path/.style={thick, to path={-| (\tikztotarget)}},
 vh path/.style={thick, to path={|- (\tikztotarget)}}]
%\begin{scope}[scale=0.75]
\node[type1] (T3)   at ( 5.6, 0) {T3};
\node[type2] (T2)   at ( 2.8, 0) {T2};
\node[type3] (T1-2) at ( 1.4, 1) {T1};
\node[type3] (T1-3) at ( 1.4,-1) {T1};
\node[type3] (T1-5) at ( 4.2, 1) {T1};
\node[type3] (T1-6) at ( 4.2,-1) {T1};
\coordinate (start) at (0  ,0);
\coordinate (end)   at (6.3,0);
\coordinate (bista) at (0.7,0);
\coordinate (biend) at (4.9,0);
\path (bista)     edge[hv path] (start)
                  edge[vh path] (T1-2.west)
                  edge[vh path] (T1-3.west)
      (T1-2.east) edge[hv path] (T2.north)
      (T1-3.east) edge[hv path] (T2.south)
      (T2.north)  edge[vh path] (T1-5.west)
      (T2.south)  edge[vh path] (T1-6.west)
      (biend)     edge[vh path] (T1-5.east)
                  edge[vh path] (T1-6.east)
                  edge[hv path] (T3.west)
      (T3.east)   edge[hv path] (end);
%\end{scope}
\end{tikzpicture}
\caption{Reliability block diagram for a `bridge' system with three component types.}
\label{fig:bridge-layout}
\end{figure}

As a toy example, consider a `bridge' type system layout with three types of components T1, T2 and T3,
as depicted in Figure~\ref{fig:bridge-layout}.
For component types T1 and T2, we consider a near-noninformative set of prior reliability functions.
For components of type T3, we consider an informative set of prior reliability functions
as given in Table~\ref{tab:bridge-T3prior}.
This set could result from eliciting prior functioning probabilities at times $0,1,2,3,4,5$ only,
and filling up the rest.
These prior assumptions, together with sets of posterior reliability functions
resulting from three different scenarios for test data for component type T3,
are illustrated in Figures~\ref{fig:bridge-fitting}, \ref{fig:bridge-early} and \ref{fig:bridge-late}; 
test data for components of type T1 and T2 are invariably taken as
$\vec{t}^1 = (2.2, 2.4, 2.6, 2.8)$ and $\vec{t}^2 = (3.2, 3.4, 3.6, 3.8)$, respectively.

\begin{table}
\centering
\begin{tabular}{crrrrr}
\toprule
$t$ & $[0,1)$ & $[1,2)$ & $[2,3)$ & $[3,4)$ & $[4,5)$ \\
\midrule
$\ytzl{3}$ & 0.625 & 0.375 & 0.250 & 0.125 & 0.010 \\
$\ytzu{3}$ & 0.999 & 0.875 & 0.500 & 0.375 & 0.250 \\
\bottomrule
\end{tabular}
\caption{Lower and upper prior functioning probability bounds for component type T3 in the `bridge' system.}
\label{tab:bridge-T3prior}
\end{table}

In Figure~\ref{fig:bridge-fitting}, 
test data for component type T3 is $\vec{t}^3 = (0.5, 1.5, 2.5, 3.5)$,
and so in line with expectations.
The posterior set of reliability functions for each component type and the whole system
is considerably smaller as compared to the prior set
(due to the low prior strength intervals
$[\ntzl{1},\ntzu{1}] = [\ntzl{2},\ntzu{2}] = [1,2]$, $[\ntzl{3},\ntzu{3}] = [1,4]$)
and so giving more precise reliability statements.
We see that posterior lower and upper functioning probabilities drop at those times $t$
when there is a failure time in the test data,
or a drop in the prior functioning probability bounds.
Note that the lower bound for prior system reliability function is zero
due to the prior lower bound of zero for T1;
for the system to function, at least two components of type T1 must function.

\begin{figure}
\includegraphics[width=\textwidth]{bridge-fittingfailures}
\caption{Prior and posterior sets of reliability functions for the `bridge' system and its three component types,
with failure times as expected for component type T3.
Test data failure times are denoted with tick marks near the time axis.}
\label{fig:bridge-fitting}
\end{figure}

In Figure~\ref{fig:bridge-early},
test data of component type T3 is instead $\vec{t}^3 = (0.6, 0.7, 0.8, 0.9)$,
and so earlier than expected.
Compared to Figure~\ref{fig:bridge-fitting},
posterior functioning intervals for T3 are wider between $t=1$ and $t=3.5$,
reflecting additional imprecision due to prior-data conflict.
For $t > 1$, it is clearly visible how $\ytnu{3}$ is halfway between $\ytzu{3}$ and $s_t^3/n_3 = 0$
(weights $\ntzu{3} = 4$ and $n_3 = 4$),
while $\ytnl{3}$ is one-fifth of $\ytzl{3}$
(weights $\ntzl{3} = 1$ and $n_3 = 4$).
Note that the posterior system functioning probability is constant for $t \in [1,2]$
because the prior functioning probability is constant for $t \in [1,2]$
and there are no failure times $\in [1,2]$ in the test data.

\begin{figure}
\includegraphics[width=\textwidth]{bridge-earlyfailures}
\caption{Prior and posterior sets of reliability functions for the `bridge' type system and its three component types,
with failure times earlier as expected for component type T3.}
\label{fig:bridge-early}
\end{figure}

In Figure~\ref{fig:bridge-late}, 
test data of component type T3 is now $\vec{t}^3 = (4.1, 4.2, 4.3, 4.4)$,
and so observed failures are later than expected.
Here we see that for $t \in [2,4]$,
posterior functioning bounds for T3 are even wider than prior functioning bounds.
The width turns back to being half the prior width
only after the four failures.
The imprecision carries over to the system bounds,
where we see wider bounds as compared to the other two scenarios
especially between $t=2$ and $t=4$.

\begin{figure}
\includegraphics[width=\textwidth]{bridge-latefailures}
\caption{Prior and posterior sets of reliability functions for the `bridge' type system and its three component types,
with failure times later as expected for component type T3.}
\label{fig:bridge-late}
\end{figure}

As example to illustrate the potential of our model,
we consider a simplified automotive brake system.
The master brake cylinder (M) activates all four wheel brake cylinders (C1 -- C4),
which in turn actuate a braking pad assembly each (P1 -- P4).
The hand brake mechanism (H) goes directly to the brake pad assemblies P3 and P4;
the car brakes when at least one brake pad assembly is actuated.
The system layout is depicted in Figure~\ref{fig:brakesystem},
together with prior and posterior sets of reliability functions for the four component types and the complete system.
Observed lifetimes from test data are indicated by tick marks in each of the four component type panels,
where $n_\text{M}=5$, $n_\text{H}=10$, $n_\text{C}=15$, and $n_\text{P}=20$.
We assume $[\ntzl{\text{M}},\ntzu{\text{M}}] = [1,8]\ \forall t$,
and $[\nktzl, \nktzu] = [1,2]$ for $k \in \{\text{H, C, P}\}$ and all $t$.
Prior functioning probability bounds for M are based on
a Weibull cdf with shape $2.5$ and scales $6$ and $8$ for the lower and upper bound, respectively.
The prior bounds for P can be seen as the least committal bounds
derived from an expert statement of $\ytz{\text{P}} \in [0.5, 0.65]$ for $t=5$ only.

***We see that posterior lower and upper functioning probabilities drop at those times $t$
for which there is a failure time in the test data,
or a drop in the prior functioning probability bounds.
The lower bound for prior system reliability drops to zero at $t=5$
since the prior lower bound for P drops to zero at $t=5$;
for the system to function, at least one brake pad assembly must function.***

For H, near-noninformative prior functioning probability bounds have been selected;
with the upper bound for P being approximately one for $t \le 5$ as well,
the prior upper system reliability bound for $t \le 5$ is close to one, too,
since the system can function on H and one of P1 -- P4 alone.
Note that the posterior functioning probability interval for M
is wide not only due to the limited number of observations,
but also because $\ntzu{\text{M}} = 8$ and the prior-data conflict reaction.

Posterior functioning probability bounds for the complete system
are much more precise than the prior system bounds,
reflecting the information gained from component test data.
The posterior system bounds can be also seen to reflect location and precision of the component bounds;
for example, the system bounds drop drastically between $t=2.5$ and $t=3.5$
mainly due to the drop of the bounds for P at that time.


\begin{figure}
\begin{tikzpicture}
[typeM/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=5mm,font=\footnotesize},
 typeC/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=5mm,font=\footnotesize},
 typeP/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=5mm,font=\footnotesize},
 typeH/.style={rectangle,draw,fill=black!20,thick,inner sep=0pt,minimum size=5mm,font=\footnotesize},
 cross/.style={cross out,draw=red,very thick,minimum width=7mm, minimum height=5mm},
 hv path/.style={thick, to path={-| (\tikztotarget)}},
 vh path/.style={thick, to path={|- (\tikztotarget)}}]
\node at (0,0) {\includegraphics[width=\textwidth]{brakingsystem-2}};
\begin{scope}[scale=1.00,xshift=3.4cm,yshift=-1.1cm]
\node[typeM] (M)    at ( 0  , 0  ) {M};
\node[typeC] (C1)   at ( 1  , 1.5) {C1};
\node[typeC] (C2)   at ( 1  , 0.5) {C2};
\node[typeC] (C3)   at ( 1  ,-0.5) {C3};
\node[typeC] (C4)   at ( 1  ,-1.5) {C4};
\node[typeP] (P1)   at ( 2  , 1.5) {P1};
\node[typeP] (P2)   at ( 2  , 0.5) {P2};
\node[typeP] (P3)   at ( 2  ,-0.5) {P3};
\node[typeP] (P4)   at ( 2  ,-1.5) {P4};
\node[typeH] (H)    at ( 0  ,-1  ) {H};
\coordinate (start)  at (-0.7, 0);
\coordinate (startC) at ( 0.5, 0);
\coordinate (startH) at (-0.4, 0);
\coordinate (Hhop1)  at ( 0.4,-1);
\coordinate (Hhop2)  at ( 0.6,-1);
\coordinate (endP)   at ( 2.5, 0);
\coordinate (end)    at ( 2.8, 0);
\path (start)     edge[hv path] (M.west)
      (M.east)    edge[hv path] (startC)
      (startC)    edge[vh path] (C1.west)
                  edge[vh path] (C2.west)
                  edge[vh path] (C3.west)
                  edge[vh path] (C4.west)
      (C1.east)   edge[hv path] (P1.west)
      (C2.east)   edge[hv path] (P2.west)
      (C3.east)   edge[hv path] (P3.west)
      (C4.east)   edge[hv path] (P4.west)
      (endP)      edge[vh path] (P1.east)
                  edge[vh path] (P2.east)
                  edge[vh path] (P3.east)
                  edge[vh path] (P4.east)
                  edge[hv path] (end)
      (startH)    edge[vh path] (H.west)
      (H.east)    edge[hv path] (Hhop1)
      (Hhop1)     edge[thick,out=90,in=90] (Hhop2)
      (Hhop2)     edge[hv path] (P3.south)
                  edge[hv path] (P4.north);
\end{scope}
\end{tikzpicture}
\caption{Prior and posterior sets of reliability functions for a simplified automotive brake system
with layout as depicted in the lower right panel.}
\label{fig:brakesystem}
\end{figure}


%***calculate and show $\Rsys(t)$ plots for a range of examples (keep system layout fixed?):
%informative prior set for some component types, different degree of imprecision,
%noninformative prior set for other components (make clear that precise `noninformative' prior actually contains info), 
%combine with prior-data conflict and non-conflict test data,
%to show effect of precision of prior knowledge / amount of data / (degree of) prior-data conflict on $\Rsys(t)$.***

%***Vacuous prior sets $[\yktzl, \yktzu] = (0,1)$ for some component types,
%informative prior sets for others. $[\nktzl, \nktzu]$ interval for all component types.***

***Some extra illustrations? E.g. the effect of extra redundancy on $\Rsys(t)$ intervals like in Risk Analysis paper?
(could nicely illustrate challenges in decision making with intervals:
do I prefer adding redundancy such that there is a chance for a large effect, but high uncertainty for it,
or do I rather add redundancy such that effect is smaller but more certain?***

***Other interesting extra illustration:
show effect of replacement of components in system at certain time points
(could be corrective (component failed) or preventive replacement),
replaced components constitute a new type with shifted component reliability function***


\section{Conclusions and Outlook}

Upscaling the survival signature to large real-world systems and networks, consisting of thousands of components, is a major challenge.
However, even for such systems the fact that one only needs to derive the survival signature once for a system is an advantage,
and also the monotonicity of the survival signature for coherent systems is very useful if one can only derive it partially.  

The survival signature and its use for uncertainty quantification for system
reliability can be generalized quite straightforwardly, mainly due to the simplicity of this concept.
For example, one may generalize the system structure function from a binary function to a probability, %\citep[see]{CCM16},
to reflect uncertainty about system functioning for known states of its components, with a further
generalization to imprecise probabilities possible. It is also conceptually easy to generalize the
survival signature to multi-state systems such that it again summarizes the structure function
in a manner that is sufficient for a range of uncertainty quantifications for the system reliability. 


***Extend the model to deal with right-censored observations which are common in the reliability setting.
The minimal assumption that a component can either fail immediately after censoring or live forever
will be simple to implement but will lead to high imprecision,
whereas assuming exchangeability with other surviving components at moment of censoring
will be more complex to accomodate but will lead to less imprecision.
This excangeability assumption lies at the core of the Kaplan-Meier estimator \citep{1958:kaplan-meier},
and has already been adopted by \citet{2004:coolen-yan} in an imprecise probability context.

***right-censoring will also allow to use data from running system,
to calculate its remaining useful life (RUL).



% ------------ bibliography -------------

\section*{References}

\bibliographystyle{elsarticle-harv}
\bibliography{ijar-npb-refs}


\section*{Appendix}
\appendix
\renewcommand*{\thesection}{\Alph{section}}

\section{Proofs}
\label{ap:proofs}

\begin{proof}[\textbf{Proof of Theorem \ref{thm:y}, p\pageref{thm:y}}]
  \label{prf:y}
  Consider the likelihood ratio for the two Beta Binomial distributions $\beta_{\yu}$ and $\beta_{\yl}$,
  \begin{align*}
    \lefteqn{\mathcal{L}(l) := \frac{p(l \mid \yu, n, m, s, N)}{p(l \mid \yl, n, m, s, N)}} \\
    &= \frac{B(l + n \yu + s, m - l + n (1 - \yu) + N - s) B(n \yl + s, n (1 - \yl) + N - s)}
            {B(n \yu + s, n (1 - \yu) + N - s) B(l + n \yl + s, m - l + n (1 - \yl) + N - s)} \\
    &= \frac{\Gamma(l + n \yu + s) \Gamma(m - l + n (1 - \yu) + N - s) \Gamma(n \yl + s) \Gamma(n (1 - \yl) + N - s)}
            {\Gamma(l + n \yl + s) \Gamma(m - l + n (1 - \yl) + N - s) \Gamma(n \yu + s) \Gamma(n (1 - \yu) + N - s)} \\
    &= \left\{ \begin{aligned}
         \frac{\prod_{x=0}^{m-1} (x + n (1 - \yu) + N - s)}
              {\prod_{x=0}^{m-1} (x + n (1 - \yl) + N - s)} &\quad\mbox{ for } l=0 \\
         \frac{\prod_{x=0}^{l-1} (x + n \yu + s) \prod_{x=0}^{m-l-1} (x + n (1 - \yu) + N - s)}
              {\prod_{x=0}^{l-1} (x + n \yl + s) \prod_{x=0}^{m-l-1} (x + n (1 - \yl) + N - s)} &\quad\mbox{ for } 0<l<m \\
         \frac{\prod_{x=0}^{m-1} (x + n \yu + s)}
              {\prod_{x=0}^{m-1} (x + n \yl + s)} &\quad\mbox{ for } l=m
       \end{aligned} \right.
  \end{align*}
  since $\Gamma(x+1)=x \Gamma(x)$.
  
  Thus,
  \begin{align*}
    \frac{\mathcal{L}(l+1)}{\mathcal{L}(l)} &=
      \frac{(l + n \yu + s) (m - l - 1 + n (1 - \yl) + N - s)}
           {(l + n \yl + s) (m - l - 1 + n (1 - \yu) + N - s)} \\
    &> 1 \quad\mbox{when}\quad 0 \le \yl < \yu \le 1
  \end{align*}
    
  Hence, $\mathcal{L}(\cdot)$ is monotone increasing for $0 < \yl < \yu < 1$, so that $\beta_{\yu}$ is larger than or equal to $\beta_{\yl}$ in monotone likelihood ratio order ($\beta_{\yu} \ge_\mathrm{lr} \beta_{\yl}$).  But, $\beta_{\yu} \ge_\mathrm{lr} \beta_{\yl} \implies \beta_{\yu} \ge_\mathrm{st} \beta_{\yl}$ (\cite[Theorem 1.C.1, p.43]{shaked2007}) giving the required result.
\end{proof}

\begin{proof}[\textbf{Proof of Theorem \ref{thm:n}, p\pageref{thm:n}}]
  \label{prf:n}
  Consider the likelihood ratio for the two Beta Binomial distributions $\beta_{\nu}$ and $\beta_{\nl}$,
  \begin{align*}
    \lefteqn{\mathcal{L}(l) := \frac{p(l \mid y, \nu, m, s, N)}{p(l \mid y, \nl, m, s, N)}} \\
    &= \frac{B(l + \nu y + s, m - l + \nu (1 - y) + N - s) B(\nl y + s, \nl (1 - y) + N - s)}
            {B(\nu y + s, \nu (1 - y) + N - s) B(l + \nl y + s, m - l + \nl (1 - y) + N - s)} \\
    &= \frac{\Gamma(l + \nu y + s) \Gamma(m - l + \nu (1 - y) + N - s)}
            {\Gamma(l + \nl y + s) \Gamma(m - l + \nl (1 - y) + N - s)} \\
    &\quad \times \frac{\Gamma(\nl y + s) \Gamma(\nl (1 - y) + N - s) \Gamma(\nu+N) \Gamma(m+\nl+N)}
                       {\Gamma(\nu y + s) \Gamma(\nu (1 - y) + N - s)\Gamma(\nl+N) \Gamma(m+\nu+N)} \\
    &= \left\{ \begin{aligned}
         \frac{\prod_{x=0}^{m-1} (x + \nu (1 - y) + N - s) \prod_{x=0}^{m-1} (x + \nl + N)}
              {\prod_{x=0}^{m-1} (x + \nl (1 - y) + N - s) \prod_{x=0}^{m-1} (x + \nu + N)} &\quad\mbox{ for } l=0 \\
         \frac{\prod_{x=0}^{l-1} (x + \nu y + s) \prod_{x=0}^{m-l-1} (x + \nu (1 - y) + N - s)}
              {\prod_{x=0}^{l-1} (x + \nl y + s) \prod_{x=0}^{m-l-1} (x + \nl (1 - y) + N - s)} \\
         \quad \times \frac{\prod_{x=0}^{m-1} (x + \nl + N)}{\prod_{x=0}^{m-1} (x + \nu + N)} &\quad\mbox{ for } 0<l<m \\
         \frac{\prod_{x=0}^{m-1} (x + \nu y + s) \prod_{x=0}^{m-1} (x + \nl + N)}
              {\prod_{x=0}^{m-1} (x + \nl y + s) \prod_{x=0}^{m-1} (x + \nu + N)} &\quad\mbox{ for } l=m
       \end{aligned} \right.
  \end{align*}
  since $\Gamma(x+1)=x \Gamma(x)$.
  
  Thus,
  \[ \frac{\mathcal{L}(l+1)}{\mathcal{L}(l)} =
      \frac{(l + \nu y + s) (m - l - 1 + \nl (1 - y) + N - s)}
           {(l + \nl y + s) (m - l - 1 + \nu (1 - y) + N - s)}
  \]
  
  However, unlike the case for the $y$ parameter in Theorem \ref{thm:y}, neither $\beta_{\nl}$ 
  nor $\beta_{\nu}$ can be guaranteed to dominate for all possible values 
  for the other parameters, so that necessary conditions for
  monotonicity (either increasing or decreasing) must be established.
  We require,
  \[ \frac{(l + \nu y + s) (m - l - 1 + \nl (1 - y) + N - s)}
          {(l + \nl y + s) (m - l - 1 + \nu (1 - y) + N - s)}
     >1
  \]
  After extensive routine algebra, this can be conveniently expressed as
  \begin{align*}
    (\nu - \nl)[y (N + m - 1) - s] - l (\nu - \nl)>0.
  \end{align*}
  This limit is hardest to satisfy for $l=m$ since $\nu-\nl>0$.
  Thus, for monotonicity to hold for all $l$, we require
  \begin{align*}
    (\nu - \nl)[y (N + m - 1) - s] - m (\nu - \nl) &>0 \\
    \implies (\nu - \nl)[y (N + m - 1) - s - m] &>0
  \end{align*}
  Since $\nu-\nl>0$ by definition, we have a monotonically increasing 
  likelihood ratio only when \[ y (N + m - 1) - s - m > 0. \]
  By a similar argument, the likelihood ratio is only monotonically decreasing when
  \[ y (N + m - 1) - s < 0. \]
  Thus,
  \[ y > \frac{s + m}{N + m - 1} \implies \beta_{\nu} \ge_{\mathrm{lr}} \beta_{\nl} \implies \beta_{\nu} \ge_{\mathrm{st}} \beta_{\nl} \]
  and 
  \[ y < \frac{s}{N + m - 1} \implies \beta_{\nu} \le_{\mathrm{lr}} \beta_{\nl} \implies \beta_{\nu} \le_{\mathrm{st}} \beta_{\nl} \]
  by \cite[Theorem 1.C.1, p.43]{shaked2007}.  In the intermediate case,
  \[ \frac{s}{N+m-1} < y < \frac{s+m}{N+m-1} \]
  we cannot definitively state the stochastic ordering on $\beta_{\nu}$ and $\beta_{\nl}$.
\end{proof}



\section{Software details}
\label{ap:software}

Functions which make it easy to use the methods of this paper have been added to the \textbf{R} package \texttt{ReliabilityTheory} \citep{2015:aslett-RT}.  There are two functions of particular note: \texttt{computeSystemSurvivalSignature} and \texttt{nonParBayesSystemInferencePriorSets}

\subsection{Computing the survival signature}

The function \texttt{computeSystemSurvivalSignature} allows easy 
computation of the survival signature if the system is expressed as an
undirected graph with `start' and `terminal' nodes (which are not
considered components for survival signature computation).  The system
is considered to work if there is a path from the start to the terminal
node passing only through functioning components.

Graph representations of systems are most simply defined by using the 
\texttt{graph.formula} function.  The `start' node should be denoted
\texttt{s} and the `terminal' node should be denoted \texttt{t} and
intermediate nodes (representing actual components) should be numbered and
connected by edges denoted by \texttt{-}, where the numbering denotes physically
distinct components.  Component numbers can be repeated to include
multiple links.  For example, to build a simple three component series
system:

\noindent\texttt{sys <- graph.formula(s\,-\,1\,-\,2\,-\,3\,-\,t)}

and to build a three component parallel system:

\noindent\texttt{sys <- graph.formula(s\,-\,1\,-\,t, s\,-\,2\,-\,t, s\,-\,3\,-\,t)}

There is an additional shorthand which indicates a link exists to a list of multiple components separated by the \texttt{:} operator, so that the parallel system can be also be expressed more compactly by:

\noindent\texttt{sys <- graph.formula(s\,-\,1:2:3\,-\,t)}

Therefore, the simple bridge system of Figure \ref{fig:bridge-layout} can be constructed with:

\noindent\texttt{sys <- graph.formula(s\,-\,1\,-\,2\,-\,3\,-\,t, s\,-\,4\,-\,5\,-\,3\,-\,t, 1:4\,-\,6\,-\,2:5)}
\begin{itemize}
  \item \texttt{s\,-\,1\,-\,2\,-\,3\,-\,t} signifies the route from left to right entering the first component going across the top of the system block diagram in Figure \ref{fig:bridge-layout};
  \item \texttt{s\,-\,4\,-\,5\,-\,3\,-\,t} signifies the bottom route through the block diagram;
  \item \texttt{1:4\,-\,6\,-\,2:5} connects the top two components of type 3 to the bottom two components of type 3, signifying the bridge.
\end{itemize}
Naturally such as expression is not necessarily unique, so that completely equivalently one may write:

\noindent\texttt{sys <- graph.formula(s\,-\,1:4\,-\,6\,-\,2:5\,-\,3\,-\,t, 1\,-\,2, 4\,-\,5)}

\ 

With the structure defined and the individual components numbered, it 
just remains to specify the types of each component.  This can be done
using the \texttt{setCompTypes} function.  This function takes the
system graph and a list of component type names (as the tag) and corresponding
component numbers (as the value).  Thus, completing the example for Figure
\ref{fig:bridge-layout}:

\noindent\texttt{sys <- setCompTypes(sys, list("T1"=c(1,2,4,5), "T2"=c(6),}

\noindent\texttt{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"T3"=c(3)))}

\ 

Computing the survival signature then involves a simple function call:
\noindent\texttt{survsig <- computeSystemSurvivalSignature(sys)}

\subsection{Computing sets of system survival probabilities}

Once the system has been correctly described using an undirected graph
as above, the methods presented in Sections \ref{sec:nonparamapproach}
-- \ref{sec:setsofrel} can be used via the function
\texttt{nonParBayesSystemInferencePriorSets}.

The function prototype is:

\noindent\texttt{nonParBayesSystemInferencePriorSets(at.times, survival.signature,}

\noindent\texttt{~~~~~~~~~~test.data, nLower=2, nUpper=2, yLower=0.5, yUpper=0.5)}

Aside from the system design, which can be passed to the function via
the \texttt{survival.signature} argument, the remaining elements which
must be specified are the:
\begin{enumerate}
  \item grid of times at which to evaluate the posterior, ${\cal T} = \{t_1, \ldots, \tmax\}$, via the \texttt{at.times} argument.
  \item component test data $\vec{t}^k = (t^k_1, \ldots, t^k_{n_k})$ for $k=1,\dots,K$, via the \texttt{test.data} argument.
  \item prior sets via the range of prior parameter sets $\PktZ = [\nktzl, \nktzu] \times [\yktzl, \yktzu]$, via the \texttt{nLower}, \texttt{nUpper}, \texttt{yLower} and \texttt{yUpper} arguments.
\end{enumerate}

The grid of times, \texttt{at.times}, is specified as simply a vector 
of time points.

The \texttt{test.data} argument is a list of component type names (as
the tag) and corresponding lifetime data (as the value), for example 
a toy sized dataset for each component would be expressed as:

\noindent\texttt{test.data=list("T1"=c(0.19, 0.73, 1.87, 1.17),}

\noindent\texttt{~~~~~~~~~~~~~~~"T2"=c(0.22, 0.27, 0.63, 1.80, 1.25, 1.95),}

\noindent\texttt{~~~~~~~~~~~~~~~"T3"=c(1.33, 0.65, 1.59))}

Finally, there are multiple options for specifying the prior parameter
sets.  Each of the \texttt{nLower}, \texttt{nUpper}, \texttt{yLower}
and \texttt{yUpper} arguments can be specified as:
\begin{itemize}
  \item a single value for a homogeneous prior across time and components.  e.g.\ \texttt{nLower=2} $\implies \nktzl=2 \ \forall\,k, t$
  \item a vector of values of length $|{\cal T}|$ (\texttt{length(at.times)}), for a time inhomogeneous prior which is identical across component types.
  \item a data frame of size $1\times K$, where each column is named the same as in the \texttt{survival.signature} and \texttt{test.data} arguments, for a time homogeneous prior which varies across component types.
  \item a data frame of size $|{\cal T}|\times K$, where each column is named the same as in the \texttt{survival.signature} and \texttt{test.data} arguments, for a time inhomogeneous prior which varies across component types.
\end{itemize}

With these arguments supplied,  \texttt{nonParBayesSystemInferencePriorSets} will then compute
the posterior sets automatically in parallel across the cores of a
multicore CPU and return a list with two objects, named \texttt{lower} 
and \texttt{upper}, containing respectively the lower and upper bound 
for the system reliability function $\Rsys(t)$.

\end{document}
