\documentclass[authoryear, 12pt, a4paper]{elsarticle}

% ------------ packages -------------

\usepackage[utf8]{inputenc}
\usepackage[OT1]{fontenc}
\usepackage{graphicx}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{url}
%\usepackage{tikz}
%\usetikzlibrary{shapes.misc,fit}


% ------------ custom defs -------------

\newcommand{\reals}{\mathbb{R}}
\newcommand{\posreals}{\reals_{>0}}
\newcommand{\posrealszero}{\reals_{\ge 0}}
\newcommand{\naturals}{\mathbb{N}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\renewcommand{\vec}[1]{{\bm#1}}

\newcommand{\uz}{^{(0)}} % upper zero
\newcommand{\un}{^{(n)}} % upper n
\newcommand{\ui}{^{(i)}} % upper i

\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ol}[1]{\overline{#1}}

\newcommand{\Rsys}{R_\text{sys}}
\newcommand{\lRsys}{\ul{R}_\text{sys}}
\newcommand{\uRsys}{\ol{R}_\text{sys}}

\newcommand{\Fsys}{F_\text{sys}}
\newcommand{\lFsys}{\ul{F}_\text{sys}}
\newcommand{\uFsys}{\ol{F}_\text{sys}}

\def\Tsys{T_\text{sys}}

\newcommand{\E}{\operatorname{E}}
\newcommand{\V}{\operatorname{Var}}

\newcommand{\indic}{\mathbb{I}}

\newcommand{\ber}{\operatorname{Bernoulli}} 
\newcommand{\bin}{\operatorname{Binomial}}
\newcommand{\be}{\operatorname{Beta}} 
\newcommand{\bebin}{\operatorname{Beta-binomial}} 

\def\tmax{t_\text{max}}
\def\tnow{t_\text{now}}
\def\tpnow{t^+_\text{now}}

\input{nydefs.tex}

%\newcommand{\comments}[1]{{\small\color{gray} #1}}

% ------------ options -------------

\allowdisplaybreaks

%\biboptions{longnamesfirst,angle,semicolon}


\journal{IJAR}

\begin{document}

% ------------ frontmatter -------------

\begin{frontmatter}
\title{IJAR Bayesian Nonparametrics Special Issue Manuscript}

\author[ein]{Gero Walter}
\ead{g.m.walter@tue.nl}
\author[oxf]{Louis J.M. Ashlett}
\ead{louis.aslett@stats.ox.ac.uk}
\author[dur]{Frank P.A. Coolen}
\ead{frank.coolen@durham.ac.uk}

\address[ein]{School of Industrial Engineering, Eindhoven University of Technology, Eindhoven, NL}
\address[oxf]{Department of Statistics, University of Oxford, Oxford, UK}
\address[dur]{Department of Mathematical Sciences, Durham University, Durham, UK}

\begin{abstract}
Imprecise Bayesian nonparametric approach to system reliability with multiple types of components,
sets of priors through sets of canonical parameters,
leading to sets of system reliability functions,
reflection of prior-data conflict
\end{abstract}

\begin{keyword}
System reliability \sep
Survival signature \sep
Imprecise probability \sep
Bayesian Nonparametrics \sep
Prior-data conflict
\end{keyword}
\end{frontmatter}


% ------------ manuscript -------------

(author order to be discussed of course)

\section{Introduction}

(some sentences may be useful for the abstract)

System with components of $k=1,\ldots,K$ different types.
There are $m_k$ components of type $k$ in the system.
Components of the same type are i.i.d.\ and independent of components of other types.
Arbitrary system layout, i.e.\ any series / parallel combination, $k$ out of $n$ etc.

Based on expert assumptions and data for component failure distribution,
the goal is to calculate the system reliability function $\Rsys(t) = P(\Tsys > t)$.
Considering a discrete grid of time points $\{t_1, \ldots, \tmax\}$,
naturally a Bayesian model for the functioning probability at each time point arises.
To account for uncertainty in the choice of prior and due to the inherent issue with prior-data conflict,
we propose an impÃ¼recise or interval probability approach
based on sets of prior distributions defined through sets of canonical parameters.

Our method produces a set of discrete system reliability functions,
or, for each time point $t$, an interval for the system survival probability
that appropriately reflects uncertainty due to vague prior information, the amount of data, and prior-data conflict.


\section{Survival Signature}

The survival signature allows for straightforward computation of system reliability
for systems with complex layouts and multiple component types.
It separates the (time-invariant) system structure from the time-dependent failure probabilities of components.

Can be seen as applying the law of total probability by partitioning
according to the number of components of type $k$ functioning at time $t$,
denoted by $C^k_t \in \{0, 1, \ldots, l_k, \ldots, m_k\}$:
\begin{align*}
P(\Tsys > t)
 &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} P(\Tsys > t \mid C^1_t = l_1,\ldots, C^K_t = l_K)
                                                 P\Big( \bigcap_{k=1}^K \{ C^k_t = l_k\} \Big) \\
 &= \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K)
                                                 \prod_{k=1}^K P(C^k_t = l_k) \,.
\end{align*}


\section{Nonparametric Bayesian Approach}

The random failure time of component number $i$ of type $k$ is denoted by $T^k_i$, $i = 1, \ldots m_k$.
For its distribution, written in terms of the cdf $F^k(t) = P(T^k_i \le t)$
or in terms of the reliability function $R^k(t) = P(T^k > t) = 1 - F^k(t)$,
we consider a set of time points $t$, $t \in {\cal T} = \{t_1, \ldots, \tmax\}$.

For each time point $t$, the functioning of a single component of type $k$ at time $t$ (functioning: 1, failed: 0)
is Bernoulli distributed with a suitable functioning probability (as, $1-$failure probability) $p^k_t$, so
\begin{align*}
P\big(\indic(T^k_i > t) = 1\big) &= p^k_t\,, \\
P\big(\indic(T^k_i > t) = 0\big) &= 1 - p^k_t\,,
\end{align*}
in short, $\indic(T^k_i > t) \sim \ber(p^k_t)$, $i = 1, \ldots, m_k$.

Then, due to the assumption of independence of components of the same type, 
the number of functioning components out of the $m_k$ components of type $k$
is binomially distributed, $C^k_t = \sum_{i=1}^{m_k} \indic(T^k_i > t) \sim \bin(p^k_t, m_k)$.

The set of functioning probabilities $p^k_t$
corresponding to a choosen set of time points ($t \in {\cal T} = \{t_1, \ldots, \tmax\}$)
defines a discrete failure time distribution for components of type $k$,
$R^k(t) = p^k_{t_j}$, $t \in [t_j, t_{j+1})$.

***or say we don't know for times between time points, or lower and upper bound already here?***
***illustrate with a graph?***

%(maybe not relevant for this manuscript, but would it make sense to take Kaplan-Meier estimates for the $p^k_t$'s,
%or the NPI bounds for Kaplan-Meier?)
%

The $p^k_t$'s can be taken such that they reflect, e.g., a bathtub curve for the corresponding hazard rate.
Naturally, $p^k_{t_j} \ge p^k_{t_{j+1}}$ should hold (assuming no repair).***

However, directly choosing the $p^k_t$'s is hard,
and fixing values for all $p^k_t$ would furthermore neglect any inherent uncertainty in the particular choice.  
To account for this uncertainty, one can express knowledge on $p^k_t$ through a prior distribution.
A convenient and natural choice is $p^k_t \sim \be(\alpha^k_t, \beta^k_t)$, a conjugate prior,
leading to the posterior being again Beta.

Having observed the lifetime data $\vec{t}^k = (t^k_1, \ldots, t^k_{n_k})$,
at any fixed time $t \in {\cal T}$ this results in the observation from the Binomial model described above,
$s^k_t = \sum_{i=1}^{n_k} \indic(t^k_i > t)$.
The posterior is then $p^k_t \mid s^k_t \sim \be(\alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)$.
The combination of a Binomial observation model with a Beta prior is often called Beta-binomial model.

The posterior predictive distribution for the number of components surviving at time $t$
of the $m_k$ components in the system, based on the lifetime data and the prior information,
is then a so-called Beta-binomial distribution,
$C^k_t \mid s^k_t \sim \bebin(m_k, \alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)$.
That is, we have
\begin{align*}
P(C^k_t = l_k \mid s^k_t) &= {m_k \choose l_k} \frac{B(l_k + \alpha^k_t + s^k_t, m_k - l_k + \beta^k_t + n_k - s^k_t)}
                                                    {B(\alpha^k_t + s^k_t, \beta^k_t + n_k - s^k_t)} \,,
\end{align*}
where $B(\cdot, \cdot)$ is the beta function.

The posterior predictive basically averages over $p^k_t \in [0,1]$,
weighted with the posterior on $p^k_t$.

For each component type $k$, we have to choose $2 \times |{\cal T}|$
parameters to specify the prior distribution for the discrete survival function of $T^k_i$,
so in total $2 \times |{\cal T}| \times K$ parameters.


Let us drop the super- and subscript $k$ and $t$ for a while
to discuss the role and interpretation of the parameters $\alpha$ and $\beta$
for each $k$ and $t$.
The Beta distribution is usually parametrized in terms of $\alpha$ and $\beta$ as used above, 
but for our generalization to sets of priors,
it is useful to consider a different parametrization in terms of $\nz$ and $\yz$, where
\begin{align*}
\nz &= \alpha + \beta &
&\text{and} &
\yz &= \frac{\alpha}{\alpha + \beta} \,,
\end{align*}
or vice versa, $\alpha = \nz\yz$ and $\beta = \nz(1-\yz)$.
$\nz$ and $\yz$ are sometimes known as the \emph{canonical} parameters,
identified from rewriting the density in a canonical form.
(This canonical form gives a joint structure to all conjugate priors.)

From the properties of the Beta distribution,
we see that $\yz = \E[p]$ is the prior expectation for the functioning probability $p$,
and that the higher $\nz$, the more probability weight will be concentrated around that mean,
as $\V(p) = \frac{\yz (1-\yz)}{\nz + 1}$.
Furthermore, $\nz$ can be interpreted as a pseudocount or prior strength,
this will be illustrated through revisiting the posterior
based on observed lifetimes $\vec{t} = (t_1, \ldots, t_{n})$:

$p \mid s$ is Beta distributed with the updated parameters
\begin{align*}
\nn &= \nz + n\,, &
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{s}{n}\,,
\end{align*}
so after seeing that $s$ out of $n$ components function (at time $t$),
the posterior mean $\yn$ for $p$ is a weighted average of
the prior mean $\yz$ and $s/n$ (the fraction of function components in the data),
with the weights $\nz$ and $n$, respectively.
We see that $\nz$ plays the same role for the prior mean $\yz$
as the sample size $n$ for the observed mean $s/n$,
thus the notion of pseudocount.
Also, the higher $\nz$, the higher the weight for $\yz$
in the weighted average calculation of $\yn$,
so $\nz$ gives the strength of the prior as compared to the sample size $n$.
%The parameters $\nz$ and $\yz$ have a more intuitive interprtation,

The Beta-binomial distribution in terms of the updated parameters $\nn$ and $\yn$,
now again with indicators $t$ and $k$,
is therefore
\begin{align*}
P(C^k_t = l_k \mid s^k_t) &= {m_k \choose l_k} \frac{B(l_k + \nn_{k,t}\yn_{k,t}, m_k - l_k + \nn_{k,t}(1-\yn_{k,t}))}
                                                    {B(\nn_{k,t}\yn_{k,t}, \nn_{k,t}(1-\yn_{k,t}))} \,.
\end{align*}

The reparametrization enables us to see that in the conjugate setting,
learning from data amounts to averaging between prior and data.
While this allows for the tractability of the model,
it also comes with a serious drawback:
When observed data are very much different from what is assumed in the prior,
this conflict is simply averaged out,
and is not reflected in the posterior or the posterior predictive.
 
***Illustration of such prior-data conflict with $\bebin$ distribution:
different priors updated to same posterior,
or same prior updated to different posteriors with same variance,
show corresponding $C^k_t$ distribution.***



\section{Sets of Priors}

As shown in Walter \& Augustin (2009),
we can have both tractability and meaningful reaction to prior-data conflict
by using sets of priors $\MktZ$ produced by parameter set $\PktZ = [\nktzl, \nktzu] \times [\yktzl, \yktzu]$. 
For such a rectangular prior parameter set $\PktZ$,
the posterior parameter set $\PktN$ is not rectangular anymore.

In case of no prior-data conflict ($s^k_t/n^k_t \in [\yktzl, \yktzu]$),
the set shrinks in the $\ykt$ dimension
(how much it shrinks depends on $\nktz$, leading to the so-called spotlight shape),
leading to smaller $\Rsys(t)$ intervals.

In case of prior-data conflict ($s^k_t/n^k_t \not\in [\yktzl, \yktzu]$),
$\PktN$ has the so-called banana shape,
giving a wider $\yktn$ interval as compared to the non-conflict case. 
As a consequence, we will have wider intervals for $\Rsys(t)$ in case of prior-data conflict.

Lower and upper bounds for $\Rsys(t)$ by min and max over $\PZ_{1,t}, \ldots, \PZ_{K,t}$.

Monotonicity in $\yz_{k,t}$, so we get
\begin{align*}
\lRsys(t)
 &= \min_{\nz_{1,t},\ldots,\nz_{K,t}} \Rsys(t) \\
 &= \min_{\nz_{1,t},\ldots,\nz_{K,t}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K)
                                                 \prod_{k=1}^K P(C^k_t = l_k \mid s^k_t) \\
 &= \min_{\nz_{1,t},\ldots,\nz_{K,t}} 
    \sum_{l_1=0}^{m_1} \cdots \sum_{l_K=0}^{m_K} \Phi(l_1, \ldots, l_K) \times \\ & \hspace*{12ex}
    \prod_{k=1}^K {m_k \choose l_k} \frac{B(l_k + \nn_{k,t}\ynl_{k,t}, m_k - l_k + \nn_{k,t}(1-\ynl_{k,t}))}
                                         {B(\nn_{k,t}\ynl_{k,t}, \nn_{k,t}(1-\ynl_{k,t}))} \,.
\end{align*}


***do we need to ensure $p^k_{t_j} \ge p^k_{t_{j+1}}$ via $\yzl_{k,t_j} \ge \yzu_{k,t_{j+1}}$?
I would say no, and for relatively dense $t$ grids, neighbouring intervals should be able to overlap after all.
What about requiring $\yzu_{k,t_j} \ge \yzu_{k,t_{j+1}}$ and $\yzl_{k,t_j} \ge \yzl_{k,t_{j+1}}$?
This might not hold for the case when we know, e.g., pretty much what's going on for low $t$
but want to be much more cautious for high $t$.***

***main contribution of the paper besides the set of priors thing:
guidelines on how to choose the parameters?***

***e.g., what to do in extreme tail of component distributions?
How sensitive is the method to choosing prior (lower, upper) mean zero for $p^k_t$ at high $t$'s?***


\section{Computations}

***publish code as seperate \textbf{R} package, or include in Louis' package \texttt{ReliabilityTheory}
(which contains the function to calculate the survival signature)***


\section{Example}

***Will ask Simme Douwe Flapper (Prof at TU Eindhoven) for real-world example,
otherwise some toy example will do.***

Show $\Rsys(t)$ intervals for prior-data conflict and non-conflict data
to show effect of prior-data conflict.

Also calculate $\Rsys(t)$ intervals for vacuous prior ($[\yktzl, \yktzu] = [0,1]$, some fixed $\nktz$)?***

Show the effect of extra redundancy on $\Rsys(t)$ intervals like in Risk Analysis paper,
or show effect of replacement of failed components in a system
(replaced components constitute a new type with shifted component reliability function)


\section{Conclusions and Outlook}


Extend the model to deal with right-censored observations which are common in realibility setting.
We assume that a minimal assumption (component can fail immediately after censoring or live forever)
will be simple to implememt but will lead to high imprecision,
whereas assuming exchangeability with other components at moment of censoring
will be more complex to accomodate but will lead to les imprecision.

\end{document}

